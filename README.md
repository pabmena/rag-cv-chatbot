# Chatbot RAG para consultar el CV/alumno (Streamlit)

Sistema de **Retrieval-Augmented Generation (RAG)** que permite consultar un CV y obtener respuestas con citas de las secciones relevantes.
Listo para ejecutar localmente con **Streamlit + FAISS + Sentence-Transformers** y generaciÃ³n vÃ­a **Ollama** (local) u **OpenAI**/ **Claude** (SaaS).

## ğŸš€ Demo rÃ¡pida (5 pasos)

```bash
# 1) Clonar o copiar esta carpeta
cd rag_cv_chatbot

# 2) Crear entorno e instalar deps
python -m venv .venv
# Windows
.venv\Scripts\activate
# Mac/Linux
source .venv/bin/activate
pip install -r requirements.txt

# 3) (Opcional) Configurar modelo de generaciÃ³n
#    OpciÃ³n A - Local con Ollama (recomendado, gratis):
#      - Instalar https://ollama.com/download
#      - Descargar modelo:  ollama pull llama3.1:8b
#      - Exportar variable:  set OLLAMA_MODEL=llama3.1:8b   (Windows)
#                            export OLLAMA_MODEL=llama3.1:8b (Mac/Linux)#
#    OpciÃ³n B - Claude:
#      - Copiar .env.example a .env y setear CLAUDE_API_KEY=<tu_api_key>
#    OpciÃ³n C - OpenAI:
#      - Copiar .env.example a .env y setear OPENAI_API_KEY=<tu_api_key>

# 4) Colocar CV en /data como PDF/TXT/MD (ej: cv_alumno.pdf).
#    TambiÃ©n podÃ©s subirlo desde la UI.
python ingest.py  # crea/actualiza el Ã­ndice

# 5) Lanzar la app
streamlit run app.py
```

Abrir el navegador en la URL que imprime Streamlit (usualmente `http://localhost:8501`).

---

## ğŸ“ Estructura

```
rag_cv_chatbot/
â”œâ”€ app.py                # UI Streamlit (chat + upload + reindex)
â”œâ”€ ingest.py             # Ingesta de documentos de /data -> Ã­ndice FAISS
â”œâ”€ rag.py                # NÃºcleo: embedder, retriever y generaciÃ³n
â”œâ”€ utils.py              # Carga y troceo de documentos
â”œâ”€ requirements.txt
â”œâ”€ .env.example
â”œâ”€ data/
â”‚  â””â”€ CV_ejemplo.md
â”œâ”€ storage/              # Se crea al ejecutar ingest.py (Ã­ndice y metadatos)
â””â”€ docs/
```

## âœ¨ CaracterÃ­sticas

- **Ingesta simple**: arrastrÃ¡ tu CV (PDF/TXT/MD) o reemplazÃ¡ el ejemplo y ejecutÃ¡ `python ingest.py`.
- **Embeddings multilingÃ¼es**: `paraphrase-multilingual-MiniLM-L12-v2` (soporta espaÃ±ol).
- **FAISS** como vector store en disco.
- **RAG** con *top-k retrieval* y citas (fragmentos + archivo origen).
- **Modelo de texto** intercambiable: **Ollama local** o **OpenAI** (selecciÃ³n automÃ¡tica por variables de entorno).
- **Streamlit** con historial de chat y subida de archivos.

## ğŸ§  Â¿CÃ³mo funciona?

1. **Troceo del CV** en fragmentos (~500 tokens con solapamiento).
2. **Embeddings** de cada chunk y construcciÃ³n del Ã­ndice **FAISS**.
3. En consulta, se recuperan los **k** fragmentos mÃ¡s similares.
4. Se genera una respuesta con un **prompt con contexto** que incluye los fragmentos recuperados.
5. Se muestran **citas** (archivo y preview del texto) para trazabilidad.

## âš™ï¸ ConfiguraciÃ³n

- Variables de entorno (en `.env`):
  - `CLAUDE_API_KEY`: clave para usar modelos de Claude-code: `claude-3-7-sonnet-20250219`
  - `OPENAI_MODEL` (opcional): por defecto `gpt-4o-mini`.
  - `OLLAMA_MODEL` (opcional): p.ej. `llama3.1:8b` si usÃ¡s Ollama local.

- ParÃ¡metros principales (editables en `rag.py`):
  - `EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"`
  - `CHUNK_SIZE = 1000` (caracteres aproximados)
  - `CHUNK_OVERLAP = 150`
  - `TOP_K = 4`

## ğŸ§ª Pruebas rÃ¡pidas de preguntas

- *"Â¿CuÃ¡l es la formaciÃ³n acadÃ©mica del alumno?"*
- *"Enumera experiencias relevantes en QA/IA con fechas."*
- *"Â¿QuÃ© habilidades tÃ©cnicas tiene y en quÃ© proyectos las aplicÃ³?"*
- *"Â¿Datos de contacto?"*

## ğŸ§¾ Licencia

MIT. 
